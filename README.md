
# Backgammon AI with Expectimax and Alpha-Beta Pruning

## üéØ Introduction
This project is an AI-powered **Backgammon game** that uses the **Expectimax algorithm** combined with **Alpha-Beta Pruning** to make optimal decisions. The AI evaluates board states based on a heuristic function and simulates possible moves to determine the best strategy.

## üöÄ Features
- üé≤ **Dice Rolling Simulation** ‚Äì Random dice rolls generate possible moves.
- ‚ôü **Board Representation** ‚Äì Tracks the positions of pieces for both players.
- üîÑ **Move Execution & Undo** ‚Äì Implements making and reversing moves for AI calculations.
- ü§ñ **Expectimax AI** ‚Äì AI searches the game tree for the best moves.
- ‚ö° **Alpha-Beta Pruning** ‚Äì Optimizes the search process by eliminating unnecessary branches.
- üìä **Heuristic Evaluation** ‚Äì Uses a scoring function to determine the best game states.

---

## üì¶ Installation
Before running the game, install the required dependencies:

```sh
pip install tabulate
```
## ‚ñ∂Ô∏è How to Run

Simply run the main script:
```sh
python main.py
```
The AI will play against itself, and the board state will be displayed after each move.

## üìÇ Project Structure
```sh
üìÇ Backgammon-AI
‚îÇ‚îÄ‚îÄ üìÑ Backgammon.py           # Abstract class defining the game structure
‚îÇ‚îÄ‚îÄ üìÑ ExpectimaxBackgammon.py # AI implementation using Expectimax algorithm
‚îÇ‚îÄ‚îÄ üìÑ main.py                 # Entry point to run the game
‚îÇ‚îÄ‚îÄ üìÑ README.md               # Project documentation
```
## üõ† How It Works

### 1Ô∏è‚É£ Board Representation

The board consists of 24 points, a bar, and bear-off areas. Pieces are stored in a list:
```sh
self.board: List[Tuple[int, Optional[str]]] = [[0, None] for _ in range(24)]
```
Each entry represents the number of pieces and the player ("white" or "black").

### 2Ô∏è‚É£ Rolling the Dice

The dice roll is simulated using:
```sh
def roll_dice(self) -> List[int]:
    return [random.randint(1, 6), random.randint(1, 6)]
```
This function generates two random numbers between 1 and 6.

### 3Ô∏è‚É£ Generating Possible Moves

The function get_possible_sequences() determines all valid moves based on the dice roll:
```sh
def get_possible_sequences(self, dice_roll: List[int]) -> List[List[Tuple[int, int]]]:
    # Generates all valid move sequences based on the dice values.
```
It accounts for blocked points, hitting opponent pieces, and bear-off conditions.

### 4Ô∏è‚É£ Move Execution & Undo

Each move modifies the game state:
```sh
def make_move(self, move: Tuple[int, int]) -> None:
    self.move_history.append((move, copy.deepcopy(self.board)))
    # Update board state based on move
```
To revert a move, the game restores the previous state:
```sh
def undo_move(self, move: Tuple[int, int]) -> None:
    self.board = self.move_history.pop()[1]
```
### 5Ô∏è‚É£ Expectimax Algorithm

The AI uses Expectimax, a variation of the Minimax algorithm that accounts for randomness in dice rolls:
```sh
def expectimax(self, depth: int, maximizing_player: bool, alpha, beta) -> float:
    if depth == 1 or self.is_game_over():
        return self.evaluate_board()

    dice_rolls = [(i, j) for i in range(1, 7) for j in range(1, 7)]
    if maximizing_player:
        max_eval = -float("inf")
        for dice_roll in dice_rolls:
            move_sequences = self.get_possible_sequences(list(dice_roll))
            for sequence in move_sequences:
                for move in sequence:
                    self.make_move(move)
                eval = self.expectimax(depth - 1, False, alpha, beta)
                for move in reversed(sequence):
                    self.undo_move(move)
                max_eval = max(max_eval, eval)
        return max_eval
    else:
        min_eval = float("inf")
        for dice_roll in dice_rolls:
            move_sequences = self.get_possible_sequences(list(dice_roll))
            for sequence in move_sequences:
                for move in sequence:
                    self.make_move(move)
                eval = self.expectimax(depth - 1, True, alpha, beta)
                for move in reversed(sequence):
                    self.undo_move(move)
                min_eval = min(min_eval, eval)
        return min_eval

‚Ä¢	The AI recursively explores possible game states.
‚Ä¢	Maximizing player aims to maximize their advantage.
‚Ä¢	Minimizing player assumes the opponent will play optimally.
‚Ä¢	It evaluates board states using a heuristic function.
```
### 6Ô∏è‚É£ Heuristic Evaluation

The AI assigns a score to each board state:
```sh
def evaluate_board(self) -> int:
    whiteScore = 0
    blackScore = 0
    for i in range(24):
        if self.board[i][1] == "white":
            whiteScore += (23 - i) * self.board[i][0]
            if self.board[i][0] == 1:  # Penalty for lonely pieces
                whiteScore += 10
        elif self.board[i][1] == "black":
            blackScore += i * self.board[i][0]
            if self.board[i][0] == 1:  # Penalty for lonely pieces
                blackScore += 10

    # Add penalties for pieces on the bar
    whiteScore += 24 * self.bar['white']
    blackScore += 24 * self.bar['black']

    # Add rewards for pieces that have been borne off
    whiteScore -= 15 * self.bear_off['white']
    blackScore -= 15 * self.bear_off['black']

    return whiteScore - blackScore if not self.is_maximizing() else blackScore - whiteScore

‚Ä¢	Lower score ‚Üí Better position for the AI.
‚Ä¢	Penalizes isolated pieces and pieces on the bar.
‚Ä¢	Rewards progress toward bear-off.
```
### 7Ô∏è‚É£ Alpha-Beta Pruning

To optimize Expectimax, Alpha-Beta Pruning reduces the number of states evaluated:
```sh
alpha = -float("inf")
beta = float("inf")

if beta <= alpha:
    break

‚Ä¢	Alpha tracks the best guaranteed value for the maximizer.
‚Ä¢	Beta tracks the best guaranteed value for the minimizer.
‚Ä¢	If Beta ‚â§ Alpha, further exploration is pruned.
```
### 8Ô∏è‚É£ Backgammon Board
üèÜ Example Output
```sh
11 10  9  8  7  6  5  4  3  2  1  0
W5 -- -- -- B3 -- B5 -- -- -- -- W2
Bar: W-0 B-0
B5 -- -- -- W3 -- W5 -- -- -- -- B2
12 13 14 15 16 17 18 19 20 21 22 23
Bear Off: White - 0, Black - 0
```
### 9Ô∏è‚É£ Future Improvements

üìå Future Improvements
	‚Ä¢	üìå Train AI with Reinforcement Learning to improve decision-making.
	‚Ä¢	üìå Add a GUI for a visual representation of the board.
	‚Ä¢	üìå Support Multiplayer Mode to allow human vs AI gameplay.

üë• Contributors

üë§ Mahsa Haghnevis ‚Äì Developer
üë§ Mahdi Mahmoudkhani ‚Äì Developer
